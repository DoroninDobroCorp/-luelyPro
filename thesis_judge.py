from __future__ import annotations

import time
from dataclasses import dataclass
from typing import Optional, Tuple, List
import threading

from loguru import logger
from dotenv import load_dotenv
from google import genai
from google.genai import types

load_dotenv()


@dataclass
class GeminiJudgeConfig:
    model_id: str = "gemini-flash-lite-latest"
    pro_model_id: str = "gemini-flash-lite-latest"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –º–æ–¥–µ–ª—å
    pro_thinking_budget: int = 8000  # Thinking budget –¥–ª—è Pro (required for thinking mode)
    max_output_tokens: int = 64
    temperature: float = 0.0
    timeout: int = 10  # –¢–∞–π–º–∞—É—Ç –¥–ª—è API –≤—ã–∑–æ–≤–∞ (—Å–µ–∫—É–Ω–¥—ã)
    max_retries: int = 1  # –£–°–ö–û–†–ï–ù–ò–ï: 1 –ø–æ–ø—ã—Ç–∫–∞ –≤–º–µ—Å—Ç–æ 3 (—ç–∫–æ–Ω–æ–º–∏–º ~2 —Å–µ–∫ –Ω–∞ —Ñ–µ–π–ª–µ)
    retry_delay: float = 0.5  # –£–°–ö–û–†–ï–ù–ò–ï: –±—ã—Å—Ç—Ä—ã–π retry (–±—ã–ª–æ 1.0)


class GeminiJudge:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ–∫—Ä—ã–≤–∞–µ—Ç –ª–∏ —Ä–µ—á—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∑–∞–¥–∞–Ω–Ω—ã–π —Ç–µ–∑–∏—Å (–¥–∞/–Ω–µ—Ç + —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å 0..1),
    –∏—Å–ø–æ–ª—å–∑—É—è Google GenAI (Gemini Flash Lite + Pro –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è).
    
    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    1. –ë—ã—Å—Ç—Ä—ã–π –≤—ã–∑–æ–≤ Gemini Flash Lite ‚Üí –º–≥–Ω–æ–≤–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
    2. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ Gemini Pro ‚Üí —É–ª—É—á—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –µ—Å–ª–∏ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è)
    
    –ú–µ—Ç—Ä–∏–∫–∏:
    - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω—ã—Ö/–Ω–µ—É–¥–∞—á–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤
    - –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ Gemini
    - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ retries
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        model_id: Optional[str] = None,
        max_output_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        enable_pro: bool = True,
        openai_api_key: Optional[str] = None,
    ) -> None:
        cfg = GeminiJudgeConfig()
        self.model_id = model_id or cfg.model_id
        self.pro_model_id = cfg.pro_model_id
        self.pro_thinking_budget = cfg.pro_thinking_budget
        self.max_output_tokens = int(max_output_tokens if max_output_tokens is not None else cfg.max_output_tokens)
        self.temperature = float(temperature if temperature is not None else cfg.temperature)
        self.timeout = cfg.timeout
        self.max_retries = cfg.max_retries
        self.retry_delay = cfg.retry_delay
        self.enable_pro = enable_pro

        import os
        key = api_key or os.getenv("GEMINI_API_KEY")
        if not key:
            raise RuntimeError("GEMINI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è GeminiJudge")
        self.client = genai.Client(api_key=key)
        
        # Fallback –Ω–∞ OpenAI
        self.openai_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        self.openai_client = None
        if self.openai_key:
            try:
                from openai import OpenAI
                self.openai_client = OpenAI(api_key=self.openai_key)
                logger.info("OpenAI fallback –¥–æ—Å—Ç—É–ø–µ–Ω")
            except ImportError:
                logger.warning("openai –ø–∞–∫–µ—Ç –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, fallback –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        self.metrics = {
            "total_calls": 0,
            "successful_calls": 0,
            "failed_calls": 0,
            "retries": 0,
            "fallback_calls": 0,
            "pro_upgrades": 0,  # –°–∫–æ–ª—å–∫–æ —Ä–∞–∑ Pro –∏–∑–º–µ–Ω–∏–ª —Ä–µ—à–µ–Ω–∏–µ Flash
            "total_time": 0.0,
        }
        
        # Callback –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –æ—Ç Pro –º–æ–¥–µ–ª–∏
        self._upgrade_callback = None
        self._last_result_container = None
        
        logger.info(f"GeminiJudge –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: model={self.model_id}, pro={self.pro_model_id}, enable_pro={enable_pro}")

    def judge(self, thesis: str, dialogue_context) -> Tuple[bool, float]:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ–∑–∏—Å–∞ —Å—Ç—É–¥–µ–Ω—Ç–æ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–∏–∞–ª–æ–≥–∞.
        
        Args:
            thesis: –¢–µ–∫—Å—Ç —Ç–µ–∑–∏—Å–∞-–ø–æ–¥—Å–∫–∞–∑–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            dialogue_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞ –≤ –æ–¥–Ω–æ–º –∏–∑ —Ñ–æ—Ä–º–∞—Ç–æ–≤:
                - str: –ø—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç (legacy API)
                - List[Tuple[str, str]]: —Å–ø–∏—Å–æ–∫ –ø–∞—Ä (—Ä–æ–ª—å, —Ç–µ–∫—Å—Ç) –≥–¥–µ:
                    * —Ä–æ–ª—å: "—Å—Ç—É–¥–µ–Ω—Ç" –∏–ª–∏ "—ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä"
                    * —Ç–µ–∫—Å—Ç: —Ä–µ–ø–ª–∏–∫–∞ —É—á–∞—Å—Ç–Ω–∏–∫–∞
                    –ü—Ä–∏–º–µ—Ä: [("—ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä", "–ö—Ç–æ –ø–µ—Ä–≤—ã–º –ø–æ–ª–µ—Ç–µ–ª –≤ –∫–æ—Å–º–æ—Å?"), 
                             ("—Å—Ç—É–¥–µ–Ω—Ç", "–ì–∞–≥–∞—Ä–∏–Ω")]
                
                –í–ê–ñ–ù–û: –ü–µ—Ä–µ–¥–∞—ë—Ç—Å—è –í–°–¨ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞ –¥–ª—è –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π
                       (Flash –∏ Pro –ø–æ–ª—É—á–∞—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ).
        
        Returns:
            Tuple[bool, float]: (–ø–æ–∫—Ä—ã—Ç_—Ç–µ–∑–∏—Å, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å_0_1)
                - covered: True –µ—Å–ª–∏ —Å—Ç—É–¥–µ–Ω—Ç –ø–µ—Ä–µ–¥–∞–ª —Å—É—Ç—å —Ç–µ–∑–∏—Å–∞ –∏–ª–∏ —Ç–µ–º–∞ —Å–º–µ–Ω–∏–ª–∞—Å—å
                - confidence: —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –æ—Ç 0.0 –¥–æ 1.0
        
        –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:
            –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤—É—é –æ—Ü–µ–Ω–∫—É:
            1. Gemini Flash Lite (–±—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç)
            2. Gemini Pro (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ, —É–ª—É—á—à–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –µ—Å–ª–∏ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è)
        """
        self.metrics["total_calls"] += 1
        start_time = time.time()
        
        if not thesis:
            return (False, 0.0)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞
        if isinstance(dialogue_context, str):
            # –°—Ç–∞—Ä—ã–π API - –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç
            dialogue_text = dialogue_context.strip()
        elif isinstance(dialogue_context, list):
            # –ù–æ–≤—ã–π API - —Å–ø–∏—Å–æ–∫ (—Ä–æ–ª—å, —Ç–µ–∫—Å—Ç)
            # –ü–µ—Ä–µ–¥–∞—ë–º –í–°–ï —Ä–µ–ø–ª–∏–∫–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            lines = []
            for role, text in dialogue_context:
                if role == "—Å—Ç—É–¥–µ–Ω—Ç":
                    lines.append(f"–°—Ç—É–¥–µ–Ω—Ç: {text}")
                else:
                    lines.append(f"–≠–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä: {text}")
            dialogue_text = "\n".join(lines)
        else:
            return (False, 0.0)
        
        if not dialogue_text:
            return (False, 0.0)

        # –ë—ã—Å—Ç—Ä—ã–π –≤—ã–∑–æ–≤ Flash Lite —Å retry
        flash_result = self._call_with_retry(
            model=self.model_id,
            thesis=thesis,
            dialogue_text=dialogue_text,
            is_pro=False
        )
        
        elapsed = time.time() - start_time
        self.metrics["total_time"] += elapsed
        
        if flash_result is None:
            # Gemini –ø–æ–ª–Ω–æ—Å—Ç—å—é —É–ø–∞–ª
            self.metrics["failed_calls"] += 1  # Gemini —É–ø–∞–ª
            
            # –ü—Ä–æ–±—É–µ–º OpenAI fallback
            logger.info("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º OpenAI fallback")
            flash_result = self._call_openai_fallback(thesis, dialogue_text)
            if flash_result is None:
                # –ò OpenAI —É–ø–∞–ª - –ø–æ–ª–Ω—ã–π –ø—Ä–æ–≤–∞–ª, –Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                return (False, 0.0)
            # OpenAI fallback —Å—Ä–∞–±–æ—Ç–∞–ª - –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            self.metrics["fallback_calls"] += 1
        
        self.metrics["successful_calls"] += 1
        covered, confidence = flash_result
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ Pro –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
        if self.enable_pro and dialogue_context:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º snapshot –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ Pro
            snapshot_context = dialogue_text
            snapshot_thesis = thesis
            snapshot_timestamp = time.time()
            
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ø–ª–∏–∫ –Ω–∞ –º–æ–º–µ–Ω—Ç –∑–∞–ø—Ä–æ—Å–∞ (–¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —á—Ç–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è)
            if isinstance(dialogue_context, list):
                snapshot_context_size = len(dialogue_context)
            else:
                snapshot_context_size = len(dialogue_context.split('\n'))
            
            # –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±–Ω–æ–≤–ª—ë–Ω Pro
            result_container = {"covered": covered, "confidence": confidence, "upgraded": False}
            
            def pro_worker():
                pro_result = self._call_with_retry(
                    model=self.pro_model_id,
                    thesis=snapshot_thesis,
                    dialogue_text=snapshot_context,
                    is_pro=True
                )
                if pro_result:
                    pro_covered, pro_conf = pro_result
                    pro_response_time = time.time()
                    
                    # –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ Pro
                    # –í—ã–∑—ã–≤–∞–µ–º callback —Å snapshot –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                    if hasattr(self, '_upgrade_callback') and self._upgrade_callback:
                        try:
                            # Callback –¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å True –µ—Å–ª–∏ –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å —Ä–µ—à–µ–Ω–∏–µ Pro
                            can_apply = self._upgrade_callback(
                                thesis=snapshot_thesis,
                                covered=pro_covered,
                                confidence=pro_conf,
                                snapshot_timestamp=snapshot_timestamp,
                                snapshot_context_size=snapshot_context_size,
                                response_timestamp=pro_response_time
                            )
                            
                            if can_apply and pro_covered != result_container["covered"]:
                                self.metrics["pro_upgrades"] += 1
                                logger.info(f"üîÑ Pro upgrade: Flash={result_container['covered']} ‚Üí Pro={pro_covered} (conf {pro_conf:.2f})")
                                result_container["covered"] = pro_covered
                                result_container["confidence"] = pro_conf
                                result_container["upgraded"] = True
                            elif not can_apply:
                                logger.debug(f"‚è≠Ô∏è  Pro –æ—Ç–≤–µ—Ç –ø—Ä–æ–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω (–∫–æ–Ω—Ç–µ–∫—Å—Ç —É—Å—Ç–∞—Ä–µ–ª): thesis={snapshot_thesis[:50]}...")
                        except Exception as e:
                            logger.error(f"Callback –æ—à–∏–±–∫–∞: {e}")
            
            thread = threading.Thread(target=pro_worker, daemon=True)
            thread.start()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –ø–æ–∑–∂–µ
            self._last_result_container = result_container
        
        return (covered, confidence)
    
    def _call_with_retry(
        self, 
        model: str, 
        thesis: str, 
        dialogue_text: str,
        is_pro: bool = False
    ) -> Optional[Tuple[bool, float]]:
        """–í—ã–∑—ã–≤–∞–µ—Ç Gemini —Å retry –∏ —Ç–∞–π–º–∞—É—Ç–æ–º."""
        sys_instr = (
            "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –Ω–∞ —ç–∫–∑–∞–º–µ–Ω–µ. –ó–ê–î–ê–ß–ê: –ø—Ä–æ–≤–µ—Ä—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ª–∏ —Å—Ç—É–¥–µ–Ω—Ç –û–¢–í–ï–¢–ò–õ –ù–ê –í–û–ü–†–û–° —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞. "
            "–¢–µ–∑–∏—Å-–ø–æ–¥—Å–∫–∞–∑–∫–∞ - —ç—Ç–æ –≠–¢–ê–õ–û–ù–ù–´–ô –û–¢–í–ï–¢ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏. "
            "–°—Ç—É–¥–µ–Ω—Ç –ù–ï –¥–æ–ª–∂–µ–Ω –ø–æ–≤—Ç–æ—Ä—è—Ç—å —Ç–µ–∑–∏—Å –¥–æ—Å–ª–æ–≤–Ω–æ! –û–Ω –¥–æ–ª–∂–µ–Ω –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å! "
            "–í–ê–ñ–ù–û - ASR (—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏) –æ—à–∏–±–∞–µ—Ç—Å—è! –ë—É–¥—å –û–ß–ï–ù–¨ —Å–Ω–∏—Å—Ö–æ–¥–∏—Ç–µ–ª—å–Ω—ã–º: "
            "1) –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ = –≤–µ—Ä–Ω–æ "
            "2) –û–ø–µ—á–∞—Ç–∫–∏/–ø—Ä–æ–ø—É—Å–∫–∏ –ø—Ä–µ–¥–ª–æ–≥–æ–≤ = –≤–µ—Ä–Ω–æ "
            "3) –°–∏–Ω–æ–Ω–∏–º—ã/–ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∫–∏ = –≤–µ—Ä–Ω–æ "
            "4) –ß–∞—Å—Ç–∏—á–Ω—ã–π –æ—Ç–≤–µ—Ç (–Ω–∞–∑–≤–∞–ª –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –∏–∑ —Ç–µ–∑–∏—Å–∞) = –≤–µ—Ä–Ω–æ "
            "5) –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–∞–¥–µ–∂/—á–∏—Å–ª–æ = –≤–µ—Ä–Ω–æ "
            "–ï—Å–ª–∏ —Å—Ç—É–¥–µ–Ω—Ç –ü–†–ê–í–ò–õ–¨–ù–û –æ—Ç–≤–µ—Ç–∏–ª –Ω–∞ –≤–æ–ø—Ä–æ—Å (–¥–∞–∂–µ –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é) - covered=true. "
            "–ï—Å–ª–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä —É—à—ë–ª –≤ –¥—Ä—É–≥—É—é —Ç–µ–º—É - covered=true. "
            "–û—Ç–≤–µ—Ç —Å—Ç—Ä–æ–≥–æ –≤ JSON: {\"covered\": true|false, \"confidence\": 0..1}"
        )
        user_prompt = (
            "–≠—Ç–∞–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç:\n" + thesis.strip() + "\n\n" +
            "–î–∏–∞–ª–æ–≥:\n" + dialogue_text + "\n\n" +
            "–ü—Ä–∞–≤–∏–ª—å–Ω–æ –ª–∏ —Å—Ç—É–¥–µ–Ω—Ç –æ—Ç–≤–µ—Ç–∏–ª –Ω–∞ –≤–æ–ø—Ä–æ—Å? (—Å—Ä–∞–≤–Ω–∏ —Å —ç—Ç–∞–ª–æ–Ω–æ–º, —É—á–∏—Ç—ã–≤–∞–π ASR-–æ—à–∏–±–∫–∏)"
        )

        # –î–ª—è Pro –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º thinking mode, –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö - –±–µ–∑ thinking
        thinking_budget = self.pro_thinking_budget if is_pro else 0
        
        cfg = types.GenerateContentConfig(
            system_instruction=sys_instr,
            max_output_tokens=self.max_output_tokens,
            temperature=self.temperature,
            top_p=0.9,
            thinking_config=types.ThinkingConfig(thinking_budget=thinking_budget) if thinking_budget > 0 else None,
        )
        
        for attempt in range(self.max_retries):
            try:
                # –í—ã–∑—ã–≤–∞–µ–º Gemini (–±–µ–∑ —Ç–∞–π–º–∞—É—Ç–∞ - signal –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –ø–æ—Ç–æ–∫–∞—Ö)
                resp = self.client.models.generate_content(
                    model=model,
                    contents=[types.Content(role="user", parts=[types.Part.from_text(text=user_prompt)])],
                    config=cfg,
                )
                raw = (resp.text or "").strip()
                
                # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
                import json
                import re
                cleaned = raw
                if '```json' in cleaned:
                    match = re.search(r'```json\s*(\{.*?\})\s*```', cleaned, re.DOTALL)
                    if match:
                        cleaned = match.group(1)
                elif '```' in cleaned:
                    match = re.search(r'```\s*(\{.*?\})\s*```', cleaned, re.DOTALL)
                    if match:
                        cleaned = match.group(1)
                
                data = json.loads(cleaned)
                covered = bool(data.get("covered", False))
                conf = float(data.get("confidence", 0.0))
                conf = max(0.0, min(1.0, conf))
                
                model_label = "Pro" if is_pro else "Flash"
                logger.debug(f"{model_label} –æ—Ç–≤–µ—Ç: covered={covered}, conf={conf:.2f}")
                return (covered, conf)
                
            except Exception as e:  # noqa: BLE001
                if attempt < self.max_retries - 1:
                    self.metrics["retries"] += 1
                    logger.warning(f"Gemini {model} attempt {attempt+1} failed: {e}, retrying...")
                    time.sleep(self.retry_delay * (attempt + 1))  # Exponential backoff
                else:
                    logger.error(f"Gemini {model} failed after {self.max_retries} attempts: {e}")
                    return None
        
        return None
    
    def _call_openai_fallback(self, thesis: str, dialogue_text: str) -> Optional[Tuple[bool, float]]:
        """Fallback –Ω–∞ OpenAI –µ—Å–ª–∏ Gemini —É–ø–∞–ª."""
        if not self.openai_client:
            return None
        
        try:
            logger.info("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º OpenAI fallback")
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": 
                     "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –Ω–∞ —ç–∫–∑–∞–º–µ–Ω–µ. –û—Ü–µ–Ω–∏ –ø–æ–∫—Ä—ã–ª –ª–∏ —Å—Ç—É–¥–µ–Ω—Ç —Ç–µ–∑–∏—Å. "
                     "–ë—É–¥—å –º—è–≥–∫–∏–º –∫ –æ–ø–µ—á–∞—Ç–∫–∞–º ASR, —Å–∏–Ω–æ–Ω–∏–º–∞–º. "
                     "–û—Ç–≤–µ—Ç –≤ JSON: {\"covered\": true|false, \"confidence\": 0..1}"},
                    {"role": "user", "content": 
                     f"–¢–µ–∑–∏—Å:\n{thesis}\n\n–î–∏–∞–ª–æ–≥:\n{dialogue_text}\n\n–ü–æ–∫—Ä—ã–ª –ª–∏ —Å—Ç—É–¥–µ–Ω—Ç —Ç–µ–∑–∏—Å?"}
                ],
                temperature=0.0,
                max_tokens=64,
                timeout=10,
            )
            
            import json
            raw = response.choices[0].message.content.strip()
            data = json.loads(raw)
            covered = bool(data.get("covered", False))
            conf = float(data.get("confidence", 0.0))
            conf = max(0.0, min(1.0, conf))
            return (covered, conf)
        except Exception as e:  # noqa: BLE001
            logger.error(f"OpenAI fallback failed: {e}")
            return None
    
    def set_upgrade_callback(self, callback):
        """
        –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç callback –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –æ—Ç Pro –º–æ–¥–µ–ª–∏.
        
        Args:
            callback: —Ñ—É–Ω–∫—Ü–∏—è —Å —Å–∏–≥–Ω–∞—Ç—É—Ä–æ–π:
                (thesis: str, covered: bool, confidence: float, 
                 snapshot_timestamp: float, snapshot_context_size: int,
                 response_timestamp: float) -> bool
                
                –î–æ–ª–∂–Ω–∞ –≤–µ—Ä–Ω—É—Ç—å True –µ—Å–ª–∏ –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å —Ä–µ—à–µ–Ω–∏–µ Pro,
                False –µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç —É—Å—Ç–∞—Ä–µ–ª (—Ç–µ–∑–∏—Å –∑–∞–∫—Ä—ã–ª—Å—è –∏–ª–∏ –ø–æ—è–≤–∏–ª–∏—Å—å –Ω–æ–≤—ã–µ —Ä–µ–ø–ª–∏–∫–∏).
                
                –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
                - thesis: —Ç–µ–∫—Å—Ç —Ç–µ–∑–∏—Å–∞ –∫–æ—Ç–æ—Ä—ã–π –æ—Ü–µ–Ω–∏–≤–∞–ª—Å—è
                - covered: —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ü–µ–Ω–∫–∏ Pro (True/False)
                - confidence: —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å Pro (0..1)
                - snapshot_timestamp: –∫–æ–≥–¥–∞ –±—ã–ª –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –∑–∞–ø—Ä–æ—Å Pro
                - snapshot_context_size: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ø–ª–∏–∫ –Ω–∞ –º–æ–º–µ–Ω—Ç –∑–∞–ø—Ä–æ—Å–∞
                - response_timestamp: –∫–æ–≥–¥–∞ –ø—Ä–∏—à—ë–ª –æ—Ç–≤–µ—Ç Pro
        """
        self._upgrade_callback = callback
    
    def get_metrics(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã —Å—É–¥—å–∏."""
        return self.metrics.copy()


__all__ = ["GeminiJudge", "GeminiJudgeConfig"]
